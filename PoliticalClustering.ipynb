{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PoliticalClustering.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pushkar-Bhuse/PrisonSystem/blob/master/PoliticalClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMceOf8cgRPh",
        "colab_type": "code",
        "outputId": "f7e7d91c-a27a-467f-99e9-6c951af3a6ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pip install mpld3\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mpld3 in /usr/local/lib/python3.6/dist-packages (0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOsKvIbh5SeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "from sklearn import feature_extraction\n",
        "import mplid3\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, words, wordnet\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gYtzEcugw5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LhxwxeEZ3Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDgN3e59Z50K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1QeC_cCMizrD7aG8VDdJpeS80dWLTja5N\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Political.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4k7l70rdYRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1W2tkrxHTSdZFs32KUznyeuMJKQ4Q1PXS\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('nonPolitical.csv')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4b8KO1WkplK",
        "colab_type": "code",
        "outputId": "d844f8a8-21ad-465b-fcf3-d4de4fc66cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "political_data = pd.read_csv(\"Political.csv\")\n",
        "nonpolitical_data = pd.read_csv(\"nonPolitical.csv\")\n",
        "\n",
        "political_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Tweets</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Since the beginning of the outbreak, retailers...</td>\n",
              "      <td>POLITICAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Out of 5 , Three belongs to #Prabh…Covering# 5...</td>\n",
              "      <td>POLITICAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>India - Muslims 15%\\nMcD going Muslims - 5%\\n....</td>\n",
              "      <td>POLITICAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>The Heritage Garden Project : building self su...</td>\n",
              "      <td>POLITICAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>It's…RT @Arun2981: People who will travel by t...</td>\n",
              "      <td>POLITICAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                             Tweets      label\n",
              "0           0  Since the beginning of the outbreak, retailers...  POLITICAL\n",
              "1           1  Out of 5 , Three belongs to #Prabh…Covering# 5...  POLITICAL\n",
              "2           2  India - Muslims 15%\\nMcD going Muslims - 5%\\n....  POLITICAL\n",
              "3           3  The Heritage Garden Project : building self su...  POLITICAL\n",
              "4           4  It's…RT @Arun2981: People who will travel by t...  POLITICAL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbJuezaPmCRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nonpolitical_data = nonpolitical_data.rename(columns={\"RT @humanvibration: GAN technology is both unimaginably complex and singularly simple. \\n\\nTrained a dataset of real inputs, like faces, the…\":\"Tweets\",\n",
        "                                  \"NONE\": \"label\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpyHP-TsiLFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.concat([political_data, nonpolitical_data], ignore_index = True)\n",
        "train_data = train_data.reindex(np.random.permutation(train_data.index))\n",
        "train_data = train_data.reset_index()\n",
        "train_data = train_data.drop(columns=['index', 'Unnamed: 0'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1JWq75jm36N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data['label'] = train_data['label'].map(lambda x : 1 if x=='POLITICAL' else 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLDSumfUnmKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHa5qseXoHEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25kshAp8rpxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
        "SAD_EMO = r\" (:'?[/|\\(]) \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV9jPq1boQne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatizing_preprocess(text):\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokenized = word_tokenize(text)\n",
        "\n",
        "  fin = [lemmatizer.lemmatize(t) for t in tokenized]\n",
        "  return \" \".join(fin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlqFMGl4rzhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replacement_patterns = [\n",
        "(r'won\\'t', 'will not'),\n",
        "(r'can\\'t', 'can not'),\n",
        "(r'i\\'m', 'i am'),\n",
        "(r'ain\\'t', 'is not'),\n",
        "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
        "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
        "(r'(\\w+)\\'s', '\\g<1> is'),\n",
        "(r'(\\w+)\\'re', '\\g<1> are'),\n",
        "(r'(\\w+)\\'d', '\\g<1> would')\n",
        "]\n",
        "\n",
        "class RegexpReplacer(object):\n",
        "\n",
        "   def __init__(self, patterns=replacement_patterns):\n",
        "     self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
        "\n",
        "   def replace(self, text):\n",
        "     s = text\n",
        "     for (pattern, repl) in self.patterns:\n",
        "       s = re.sub(pattern, repl, s)\n",
        "\n",
        "     return s\n",
        "\n",
        "\n",
        "class RepeatReplacer(object):\n",
        "    def __init__(self):\n",
        "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "        self.repl = r'\\1\\2\\3'\n",
        "    def replace(self, word):\n",
        "        if wordnet.synsets(word):\n",
        "            return word\n",
        "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
        "        if repl_word != word:\n",
        "            return self.replace(repl_word)\n",
        "        else:\n",
        "            return repl_word\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbKUwK12slFl",
        "colab_type": "code",
        "outputId": "8a40669a-08c6-4ebf-9354-cf60074540f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppjIuMT5sHAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "def remove_unknown(text):\n",
        "  printable = set(string.printable)\n",
        "  return ''.join(filter(lambda x: x in printable, text))\n",
        "\n",
        "seperate_words = RegexpReplacer()\n",
        "\n",
        "repeated_letters = RepeatReplacer()\n",
        "\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].str.replace(r\"[-\\.\\n]\", \"\")\n",
        "# Removing HTML garbage\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].str.replace(r\"&\\w+;\", \"\")\n",
        "# Removing links\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].str.replace(r\"https?://\\S*\", \"\")\n",
        "\n",
        "# mark emoticons as happy or sad\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].str.replace(HAPPY_EMO, \" happy \")\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].str.replace(SAD_EMO, \" sad \")\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].str.lower()\n",
        "\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].apply(remove_unknown)\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].apply(lambda x : seperate_words.replace(x))\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].apply(lambda x : repeated_letters.replace(x))\n",
        "\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].str.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "#Handling tags, In this case we remove all the tags\n",
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].str.replace(r\"@[a-zA-Z0-9_]* \", \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhppLnQ7saL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data[\"Tweets\"] = train_data[\"Tweets\"].apply(lemmatizing_preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfqMWc4Zut7h",
        "colab_type": "code",
        "outputId": "9dd5038a-8ab8-43b7-cf27-99b113f913e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweets</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rt mathurvaishali my coleague shahbaz who desi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>whereisosha this story is so pencedemic so sti...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>space nasa ufo spacex spacexploration mondayth...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>barbados matwhirpol now we realy have to go ba...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>like for surjewala</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Tweets  label\n",
              "0  rt mathurvaishali my coleague shahbaz who desi...      0\n",
              "1  whereisosha this story is so pencedemic so sti...      0\n",
              "2  space nasa ufo spacex spacexploration mondayth...      1\n",
              "3  barbados matwhirpol now we realy have to go ba...      0\n",
              "4                                 like for surjewala      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P757ogHptL0o",
        "colab_type": "code",
        "outputId": "3a5400f7-660d-4ec8-b774-2db214e6d562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tfidf_tokenizer(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
        "                                 min_df=0.2, use_idf=True, tokenizer=tfidf_tokenizer, ngram_range=(1,3))\n",
        "\n",
        "%time tfidf_matrix = tfidf_vectorizer.fit_transform(train_data[\"Tweets\"].tolist())\n",
        "\n",
        "print(tfidf_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 30.6 s, sys: 340 ms, total: 30.9 s\n",
            "Wall time: 30.9 s\n",
            "(75148, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bob296EGtkfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terms = tfidf_vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ-RoxwctwaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dist = 1 - cosine_similarity(tfidf_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C39jyAGvcDLD",
        "colab_type": "text"
      },
      "source": [
        "#Option 1: KNN Clustering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlUCmDP3wTok",
        "colab_type": "code",
        "outputId": "a969f3dc-1b61-49d0-842c-e85ba8991528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 2\n",
        "\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "\n",
        "%time km.fit(tfidf_matrix)\n",
        "\n",
        "clusters = km.labels_.tolist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 28.5 s, sys: 84.8 ms, total: 28.5 s\n",
            "Wall time: 28.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PwZX0ViVjPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = { 'label': train_data['label'], 'Tweets': train_data['Tweets'], 'cluster': clusters }\n",
        "\n",
        "frame = pd.DataFrame(data, index = [clusters] , columns = ['label', 'Tweets', 'cluster'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXHZ6qLXZW_7",
        "colab_type": "code",
        "outputId": "2f48d759-64e2-4d82-f325-ee00cdb00550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "frame['cluster'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    38010\n",
              "1    37138\n",
              "Name: cluster, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t16IHIizZbGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "def strip_proppers(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
        "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r18ZNjKZhPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "def strip_proppers_POS(text):\n",
        "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
        "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
        "    return non_propernouns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lygqQL_2bz3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKZJJREsb18I",
        "colab_type": "text"
      },
      "source": [
        "#Option 2: Latent Dirichlet Allocation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21hmEk3iadlg",
        "colab_type": "code",
        "outputId": "e8c675a8-28f5-40d6-d65a-6d17b405c9af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Latent Dirichlet Allocation implementation with Gensim\n",
        "\n",
        "from gensim import corpora, models, similarities \n",
        "\n",
        "#remove proper names\n",
        "preprocess = [strip_proppers(doc) for doc in train_data['Tweets']]\n",
        "\n",
        "%time tokenized_text = [tfidf_tokenizer(lemmatizing_preprocess(text)) for text in preprocess]\n",
        "\n",
        "%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 41.5 s, sys: 280 ms, total: 41.8 s\n",
            "Wall time: 41.8 s\n",
            "CPU times: user 4.97 s, sys: 13 ms, total: 4.98 s\n",
            "Wall time: 4.98 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdjrbMN1ank6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = corpora.Dictionary(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alh-bz9XbPNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary.filter_extremes(no_below=1, no_above=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg8w1mdZbTVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi0PWL-mbVqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time lda = models.LdaModel(corpus, num_topics=2, id2word=dictionary, update_every=5, chunksize=10000, passes=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JIFRW_xbX7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lda[corpus[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puHYMinbbfBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topics = lda.print_topics(2, num_words=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVqBW3Z7booW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topics_matrix = lda.show_topics(formatted=False, num_words=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymrYSFQRbpzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topics_matrix = np.array(topics_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C03JNrtbsRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_words = topics_matrix[:,:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lOAroBMbwKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in topic_words:\n",
        "    print([str(word) for word in i])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y3ZVs2ScPBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}